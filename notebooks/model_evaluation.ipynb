{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d597b4c-76a5-48c3-a520-96d5c968e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O         : 0.0001\n",
      "B-Product : 0.2097\n",
      "I-Product : 0.0290\n",
      "B-PRICE   : 0.2996\n",
      "I-PRICE   : 0.2097\n",
      "B-LOC     : 0.2247\n",
      "I-LOC     : 0.0272\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# Label counts you already obtained\n",
    "label_counts = {\n",
    "    \"O\": 46919,\n",
    "    \"B-Product\": 30,\n",
    "    \"I-Product\": 217,\n",
    "    \"B-PRICE\": 21,\n",
    "    \"I-PRICE\": 30,\n",
    "    \"B-LOC\": 28,\n",
    "    \"I-LOC\": 231\n",
    "}\n",
    "\n",
    "# Convert to list with label2id order\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Product\": 1,\n",
    "    \"I-Product\": 2,\n",
    "    \"B-PRICE\": 3,\n",
    "    \"I-PRICE\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}\n",
    "\n",
    "# Total number of samples\n",
    "total = sum(label_counts.values())\n",
    "num_classes = len(label_counts)\n",
    "\n",
    "# Calculate weights\n",
    "weights = [total / (num_classes * label_counts[label]) for label in label2id.keys()]\n",
    "\n",
    "# Normalize weights (optional but can help stability)\n",
    "weights = torch.tensor(weights)\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "# Print final weights\n",
    "for label, weight in zip(label2id.keys(), weights):\n",
    "    print(f\"{label:10s}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f3495d-3c14-49d6-8d20-747cd6646932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Tag Distribution:\n",
      "B-Product : 30\n",
      "I-Product : 217\n",
      "O         : 46919\n",
      "B-PRICE   : 21\n",
      "I-PRICE   : 30\n",
      "B-LOC     : 28\n",
      "I-LOC     : 231\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 1: Label mappings\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Product\": 1,\n",
    "    \"I-Product\": 2,\n",
    "    \"B-PRICE\": 3,\n",
    "    \"I-PRICE\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Step 2: Define parse_conll (copy this from your original script)\n",
    "def parse_conll(file_path):\n",
    "    sentences = []\n",
    "    tokens, tags = [], []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                if len(line.split()) == 2:\n",
    "                    token, tag = line.split()\n",
    "                    tokens.append(token)\n",
    "                    tags.append(label2id.get(tag, 0))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Step 3: Load and count tags\n",
    "parsed_data = parse_conll(\"../data/clean/labeled_conll.txt\")\n",
    "all_tags = [tag for sample in parsed_data for tag in sample[\"ner_tags\"]]\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# Step 4: Print tag stats\n",
    "print(\"\\nNER Tag Distribution:\")\n",
    "for tag_id, count in tag_counts.items():\n",
    "    print(f\"{id2label[tag_id]:<10}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac011b3-2425-4037-8fb7-55b59eab6361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tag Distribution:\n",
      "B-Product : 30\n",
      "I-Product : 217\n",
      "O         : 46919\n",
      "B-PRICE   : 21\n",
      "I-PRICE   : 30\n",
      "B-LOC     : 28\n",
      "I-LOC     : 231\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 1: Define your label mappings\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Product\": 1,\n",
    "    \"I-Product\": 2,\n",
    "    \"B-PRICE\": 3,\n",
    "    \"I-PRICE\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Step 2: Define the parser to load your dataset\n",
    "def parse_conll(file_path):\n",
    "    sentences = []\n",
    "    tokens, tags = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                if len(line.split()) == 2:\n",
    "                    token, tag = line.split()\n",
    "                    tokens.append(token)\n",
    "                    tags.append(label2id.get(tag, 0))\n",
    "    return sentences\n",
    "\n",
    "# Step 3: Count label frequencies\n",
    "def count_labels(dataset):\n",
    "    counter = Counter()\n",
    "    for entry in dataset:\n",
    "        for tag in entry['ner_tags']:\n",
    "            counter[tag] += 1\n",
    "    return {id2label[k]: v for k, v in counter.items()}\n",
    "\n",
    "# Step 4: Load and count\n",
    "dataset = parse_conll(\"../data/clean/labeled_conll.txt\")\n",
    "counts = count_labels(dataset)\n",
    "\n",
    "# Step 5: Print results\n",
    "print(\"NER Tag Distribution:\")\n",
    "for label, count in counts.items():\n",
    "    print(f\"{label:<10}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da60848a-af42-469b-8d2f-87c3c9dd8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ddba7e6-9e86-4a09-b902-b06ff282ae66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ad172497164f8e9c008eb6a6458c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6bfa39c74647c386e2fcf290c19fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9235296c41c9433db1d83a24203c1d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"eval_loss\": 4.736419200897217,\n",
      "    \"eval_model_preparation_time\": 0.0043,\n",
      "    \"eval_precision\": 0.0,\n",
      "    \"eval_recall\": 0.0,\n",
      "    \"eval_f1\": 0.0,\n",
      "    \"eval_accuracy\": 0.5555555555555556,\n",
      "    \"eval_runtime\": 0.6814,\n",
      "    \"eval_samples_per_second\": 1.468,\n",
      "    \"eval_steps_per_second\": 1.468\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2}, 'PRICE': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'Product': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.42857142857142855}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Label mappings\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Product\": 1,\n",
    "    \"I-Product\": 2,\n",
    "    \"B-PRICE\": 3,\n",
    "    \"I-PRICE\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "# --- IMPORTANT: You need to have 'tokenized_dataset' defined here ---\n",
    "# This part is missing in your provided code snippet.\n",
    "# 'tokenized_dataset' should be a Hugging Face Dataset object,\n",
    "# typically obtained after tokenizing your raw data.\n",
    "# For demonstration purposes, let's assume you have a dummy one or load it:\n",
    "\n",
    "# Example of how you might get tokenized_dataset (replace with your actual data loading/tokenization)\n",
    "# If you have a pickled tokenized_dataset from a previous step, load it:\n",
    "# with open(\"path/to/your/tokenized_dataset.pkl\", \"rb\") as f:\n",
    "#     tokenized_dataset = pickle.load(f)\n",
    "\n",
    "# Or create a dummy one for the sake of making the script runnable for demonstration\n",
    "# In a real scenario, this would come from your data processing pipeline.\n",
    "raw_data = [\n",
    "    {\"tokens\": [\"This\", \"is\", \"a\", \"test\", \"product\", \"for\", \"10\", \"at\", \"Addis\"],\n",
    "     \"ner_tags\": [0, 0, 0, 0, 1, 0, 3, 0, 5]},\n",
    "    {\"tokens\": [\"Another\", \"item\", \"price\", \"5\", \"in\", \"Gonder\"],\n",
    "     \"ner_tags\": [0, 1, 0, 3, 0, 5]}\n",
    "]\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx]) # or -100 for subsequent tokens of a word\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load tokenizer early for dummy dataset creation\n",
    "model_path = \"../models/xlmr-ner-amharic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Convert dummy raw data to Dataset and then tokenize\n",
    "dummy_dataset = Dataset.from_list(raw_data)\n",
    "tokenized_dataset = dummy_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Split and save datasets to disk *before* attempting to load them\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']\n",
    "\n",
    "# Save datasets to disk\n",
    "train_dataset.save_to_disk(\"train_dataset\")\n",
    "eval_dataset.save_to_disk(\"eval_dataset\")\n",
    "\n",
    "# Now, load the eval_dataset from disk\n",
    "# eval_dataset = load_from_disk(\"eval_dataset\") # This line is now redundant if you directly use the 'eval_dataset' created above\n",
    "\n",
    "# --- End of 'tokenized_dataset' handling ---\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "# model_path = \"models/xlmr-ner-amharic\" # Already defined above\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path) # Already loaded above\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "\n",
    "\n",
    "# Load metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Compute metrics function to use with Trainer\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[label] if label != -100 else \"O\" for label in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_preds = [\n",
    "        [id2label[pred] for pred in pred_seq]\n",
    "        for pred_seq in predictions\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Create Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../outputs/ner-xlmr\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset, # Use the 'eval_dataset' created directly\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "metrics = trainer.evaluate()\n",
    "print(json.dumps(metrics, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Assuming you have these already loaded\n",
    "# model: your fine-tuned model\n",
    "# eval_dataset: your evaluation dataset\n",
    "# id2label: mapping from label ids to label names\n",
    "\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "# Get predictions and label ids from the model on eval dataset\n",
    "outputs = trainer.predict(eval_dataset)\n",
    "\n",
    "logits = outputs.predictions  # shape: (num_samples, max_seq_length, num_labels)\n",
    "labels = outputs.label_ids    # shape: (num_samples, max_seq_length)\n",
    "\n",
    "# Convert logits to predicted class indices\n",
    "predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "# Load metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Convert label ids and predictions to label names, ignoring special tokens (-100)\n",
    "true_labels = [\n",
    "    [id2label[label] for label in label_seq if label != -100]\n",
    "    for label_seq in labels\n",
    "]\n",
    "\n",
    "predicted_labels = [\n",
    "    [id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "    for pred_seq, label_seq in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "# Compute metrics\n",
    "results = metric.compute(predictions=predicted_labels, references=true_labels)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716efe5b-1260-4b86-a8fe-f13317816351",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"O\", 1: \"B-Product\", 2: \"I-Product\", 3: \"B-PRICE\", 4: \"I-PRICE\", 5: \"B-LOC\", 6: \"I-LOC\"}\n",
    "label2id = {label: id for id, label in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee136ae-bbb2-4207-9456-01f50ed57659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForTokenClassification\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-Product\",\n",
    "    2: \"I-Product\",\n",
    "    3: \"B-PRICE\",\n",
    "    4: \"I-PRICE\",\n",
    "    5: \"B-LOC\",\n",
    "    6: \"I-LOC\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c876c1ac-1184-43bf-918f-8db752f0c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tag Distribution:\n",
      "B-Product : 30\n",
      "I-Product : 217\n",
      "O         : 46919\n",
      "B-PRICE   : 21\n",
      "I-PRICE   : 30\n",
      "B-LOC     : 28\n",
      "I-LOC     : 231\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 1: Define your label mappings\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Product\": 1,\n",
    "    \"I-Product\": 2,\n",
    "    \"B-PRICE\": 3,\n",
    "    \"I-PRICE\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Step 2: Define the parser to load your dataset\n",
    "def parse_conll(file_path):\n",
    "    sentences = []\n",
    "    tokens, tags = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                if len(line.split()) == 2:\n",
    "                    token, tag = line.split()\n",
    "                    tokens.append(token)\n",
    "                    tags.append(label2id.get(tag, 0))\n",
    "    return sentences\n",
    "\n",
    "# Step 3: Count label frequencies\n",
    "def count_labels(dataset):\n",
    "    counter = Counter()\n",
    "    for entry in dataset:\n",
    "        for tag in entry['ner_tags']:\n",
    "            counter[tag] += 1\n",
    "    return {id2label[k]: v for k, v in counter.items()}\n",
    "\n",
    "# Step 4: Load and count\n",
    "dataset = parse_conll(\"../data/clean/labeled_conll.txt\")\n",
    "counts = count_labels(dataset)\n",
    "\n",
    "# Step 5: Print results\n",
    "print(\"NER Tag Distribution:\")\n",
    "for label, count in counts.items():\n",
    "    print(f\"{label:<10}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522377e9-d0ff-4f19-aed0-362f5873a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tag Distribution:\n",
      "B-Product : 69\n",
      "I-Product : 143\n",
      "O         : 42418\n",
      "B-PRICE   : 243\n",
      "I-PRICE   : 201\n",
      "B-LOC     : 1923\n",
      "I-LOC     : 2476\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 1: Define your label mappings\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Product\": 1,\n",
    "    \"I-Product\": 2,\n",
    "    \"B-PRICE\": 3,\n",
    "    \"I-PRICE\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Step 2: Define the parser to load your dataset\n",
    "def parse_conll(file_path):\n",
    "    sentences = []\n",
    "    tokens, tags = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                if len(line.split()) == 2:\n",
    "                    token, tag = line.split()\n",
    "                    tokens.append(token)\n",
    "                    tags.append(label2id.get(tag, 0))\n",
    "    return sentences\n",
    "\n",
    "# Step 3: Count label frequencies\n",
    "def count_labels(dataset):\n",
    "    counter = Counter()\n",
    "    for entry in dataset:\n",
    "        for tag in entry['ner_tags']:\n",
    "            counter[tag] += 1\n",
    "    return {id2label[k]: v for k, v in counter.items()}\n",
    "\n",
    "# Step 4: Load and count\n",
    "dataset = parse_conll(\"../auto_labeled_conll.txt\")\n",
    "counts = count_labels(dataset)\n",
    "\n",
    "# Step 5: Print results\n",
    "print(\"NER Tag Distribution:\")\n",
    "for label, count in counts.items():\n",
    "    print(f\"{label:<10}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363bfb21-931d-4e62-aad3-021b0e98b25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
